{"paragraphs":[{"text":"%md\n\n# Learning Spark\nApache Spark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. [wiki](https://en.wikipedia.org/wiki/Apache_Spark)\n\n## Learning Objective\nWe will first learn single node RDD like another data structure. We will then introduce the distributed mode and talk about distributed variables. Eventually, we will study how to use higher-level tools like Spark SQL and Dataframe.\n\n- Spark RDD\n    - Focus on RDD operations(transformations/actions) as a data structure\n- Spark Architecture and cluster mode\n    - Focus on Spark cluster mode\n    - Optimization\n- Spark SQL and Dataframe\n    - Familiar with high-level tools that replace RDD\n- Spark structured streaming\n\n## Learning Spark RDD\n\nIn this section, we will only focus on single node RDD like another data structure. We will learn how to create and manipulate RDDs without worrying how does it work in a distributed system. In the next section, we will study the Spark architecture and different deploy modes.\n\n**Readings**\n\n- `Spark: The Definitive Guide: Big Data Processing Made Simple` (`SDG`) Chapter 12 & 13\n- Spark RDD Doc: https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html\n- RDD paper [link](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n\nPlease try to learn Spark and Scala using **Scala Docs**\n\n- Scala: https://www.scala-lang.org/api/2.11.8/#package\n- Spark Scala Doc: https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\n\n**Definition**\nAn RDD is a resilient and distributed collection of records spread over one or many partitions.\n\n- Resilient, i.e. fault-tolerant with the help of RDD lineage graph and so able to recompute missing or damaged partitions due to node failures.\n- Distributed with data residing on multiple nodes in a cluster.\n- Dataset is a collection of partitioned data with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with).\n\n**Creating RDDs**\n\n```scala\n//create an RDD from a exiting data structure\nval listRdd = SparkContext.parallelize(List(1,2,3))\n\n//create an RDD from data sources \nspark.sparkContext.textFile(\"/tmp/myTest.txt\")\nspark.sparkContext.textFile(\"hdfs:///tmp/myTest.txt\")\n```\n\n**Manipulating RDDs**\nRDDs support two kinds of operations:\n\n- transformations - lazy operations that return another RDD.\n- actions - operations that trigger computation and return values.","user":"anonymous","dateUpdated":"2019-09-25T17:24:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Learning Spark</h1>\n<p>Apache Spark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley&rsquo;s AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. <a href=\"https://en.wikipedia.org/wiki/Apache_Spark\">wiki</a></p>\n<h2>Learning Objective</h2>\n<p>We will first learn single node RDD like another data structure. We will then introduce the distributed mode and talk about distributed variables. Eventually, we will study how to use higher-level tools like Spark SQL and Dataframe.</p>\n<ul>\n  <li>Spark RDD\n    <ul>\n      <li>Focus on RDD operations(transformations/actions) as a data structure</li>\n    </ul>\n  </li>\n  <li>Spark Architecture and cluster mode\n    <ul>\n      <li>Focus on Spark cluster mode</li>\n      <li>Optimization</li>\n    </ul>\n  </li>\n  <li>Spark SQL and Dataframe\n    <ul>\n      <li>Familiar with high-level tools that replace RDD</li>\n    </ul>\n  </li>\n  <li>Spark structured streaming</li>\n</ul>\n<h2>Learning Spark RDD</h2>\n<p>In this section, we will only focus on single node RDD like another data structure. We will learn how to create and manipulate RDDs without worrying how does it work in a distributed system. In the next section, we will study the Spark architecture and different deploy modes.</p>\n<p><strong>Readings</strong></p>\n<ul>\n  <li><code>Spark: The Definitive Guide: Big Data Processing Made Simple</code> (<code>SDG</code>) Chapter 12 &amp; 13</li>\n  <li>Spark RDD Doc: <a href=\"https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html\">https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html</a></li>\n  <li>RDD paper <a href=\"https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf\">link</a></li>\n</ul>\n<p>Please try to learn Spark and Scala using <strong>Scala Docs</strong></p>\n<ul>\n  <li>Scala: <a href=\"https://www.scala-lang.org/api/2.11.8/#package\">https://www.scala-lang.org/api/2.11.8/#package</a></li>\n  <li>Spark Scala Doc: <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD</a></li>\n</ul>\n<p><strong>Definition</strong><br/>An RDD is a resilient and distributed collection of records spread over one or many partitions.</p>\n<ul>\n  <li>Resilient, i.e. fault-tolerant with the help of RDD lineage graph and so able to recompute missing or damaged partitions due to node failures.</li>\n  <li>Distributed with data residing on multiple nodes in a cluster.</li>\n  <li>Dataset is a collection of partitioned data with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with).</li>\n</ul>\n<p><strong>Creating RDDs</strong></p>\n<pre><code class=\"scala\">//create an RDD from a exiting data structure\nval listRdd = SparkContext.parallelize(List(1,2,3))\n\n//create an RDD from data sources \nspark.sparkContext.textFile(&quot;/tmp/myTest.txt&quot;)\nspark.sparkContext.textFile(&quot;hdfs:///tmp/myTest.txt&quot;)\n</code></pre>\n<p><strong>Manipulating RDDs</strong><br/>RDDs support two kinds of operations:</p>\n<ul>\n  <li>transformations - lazy operations that return another RDD.</li>\n  <li>actions - operations that trigger computation and return values.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403834_1020275360","id":"20190920-182115_2130024632","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:24:45+0000","dateFinished":"2019-09-25T17:24:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:194"},{"text":"%md\n### How to interact with Spark\n\nTo start a Spark job (either single JVM or distributed mode), we can simply execute `bin/spark-shell` cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)\n\n> Compare SparkContext and SparkSession https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\n\nAlternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (`spark`) and a SparkContext (`sc`) for you.\n","user":"anonymous","dateUpdated":"2019-09-25T17:24:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to interact with Spark</h3>\n<p>To start a Spark job (either single JVM or distributed mode), we can simply execute <code>bin/spark-shell</code> cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)</p>\n<blockquote>\n  <p>Compare SparkContext and SparkSession <a href=\"https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\">https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</blockquote>\n<p>Alternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (<code>spark</code>) and a SparkContext (<code>sc</code>) for you.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403836_-144993530","id":"20190921-014743_1530188134","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:24:49+0000","dateFinished":"2019-09-25T17:24:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:195"},{"text":"%spark\n\n//Spark session and sparkContext are loaded automatically\nprintln(spark.version.to)\nprintln(spark)\n\n//The following two lines point to the same SparkContext@2a695829 where @2a695829 is the memory address\nprintln(spark.sparkContext)\nprintln(sc)\n","user":"anonymous","dateUpdated":"2019-09-25T17:24:49+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"lineNumbers":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2.3.3\norg.apache.spark.sql.SparkSession@30d174d2\norg.apache.spark.SparkContext@600ff23a\norg.apache.spark.SparkContext@600ff23a\n"}]},"apps":[],"jobName":"paragraph_1569244403837_-271773266","id":"20190921-013657_404311467","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:24:49+0000","dateFinished":"2019-09-25T17:24:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:196"},{"text":"%md","user":"anonymous","dateUpdated":"2019-09-25T17:24:50+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403837_1535938326","id":"20190922-220218_788870347","dateCreated":"2019-09-23T13:13:23+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:197"},{"text":"%md\n### Creating RDDs from Scala collections\n\nWe can use `sc.parallelize` method to create RDDs from Scala collections\n(Note: `parallelize` is not available in the `SparkSession`. However, you can use `SparkSession.sparkContext.parallelize` instead)\n\nhttps://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext","user":"anonymous","dateUpdated":"2019-09-25T17:24:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Scala collections</h3>\n<p>We can use <code>sc.parallelize</code> method to create RDDs from Scala collections<br/>(Note: <code>parallelize</code> is not available in the <code>SparkSession</code>. However, you can use <code>SparkSession.sparkContext.parallelize</code> instead)</p>\n<p><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403837_-2139107719","id":"20190921-022812_325072599","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:24:51+0000","dateFinished":"2019-09-25T17:24:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:198"},{"text":"%spark\n//Create RDDs from Scala collections\nval lsRdd = sc.parallelize(List(1,2,3,4,5))\n\n//number of items in lsRdd\nval count = lsRdd.count\n\n//first element in lsRdd\nval firstE = lsRdd.first\n\n//Number of partitions\nval partitionsNum = lsRdd.partitions.length\n\n//Manipulating lsRDD\nval dupRdd = lsRdd.flatMap(i => List.fill(i)(i))\nval dupArray = dupRdd.collect\nval evens = dupRdd.filter(_%2 == 0).collect","user":"anonymous","dateUpdated":"2019-09-25T17:24:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lsRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at <console>:27\ncount: Long = 5\nfirstE: Int = 1\npartitionsNum: Int = 2\ndupRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[5] at flatMap at <console>:29\ndupArray: Array[Int] = Array(1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5)\nevens: Array[Int] = Array(2, 2, 4, 4, 4, 4)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=1","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=2","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=3","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=4"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403837_-1397035310","id":"20190921-020350_225494359","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:24:52+0000","dateFinished":"2019-09-25T17:24:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:199"},{"text":"%md","user":"anonymous","dateUpdated":"2019-09-25T17:24:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403838_105631293","id":"20190922-220230_613999600","dateCreated":"2019-09-23T13:13:23+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:200"},{"text":"%md\n### Creating RDDs from Data source\n\n- `ssh` to dataproc master node\n- Download `online-retail-dataset.txt` dataset [link](https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv)\n- Upload `online-retail-dataset.txt` to a HDFS location (e.g. hdfs dfs -put ...)\n- Inspect the dataset using spark RDD (see below Spark code)\n- Discuss why there are some lines that have more than 8 columns (hint: csv format)\n- Discuss some possible solutions ","user":"anonymous","dateUpdated":"2019-09-25T17:24:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Data source</h3>\n<ul>\n  <li><code>ssh</code> to dataproc master node</li>\n  <li>Download <code>online-retail-dataset.txt</code> dataset <a href=\"https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv\">link</a></li>\n  <li>Upload <code>online-retail-dataset.txt</code> to a HDFS location (e.g. hdfs dfs -put &hellip;)</li>\n  <li>Inspect the dataset using spark RDD (see below Spark code)</li>\n  <li>Discuss why there are some lines that have more than 8 columns (hint: csv format)</li>\n  <li>Discuss some possible solutions</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403838_-768173862","id":"20190920-182511_1653833929","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:24:55+0000","dateFinished":"2019-09-25T17:24:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:201"},{"text":"%spark\nval retailRDD = sc.textFile(\"hdfs:///user/miriam/datasets/online_retail/online-retail-dataset.txt\")\n\n//count number of elements in the RDD\nretailRDD.count\n\n//understand what does each element look like in RDD\nval firstE = retailRDD.first()\n\n//find out does withReplacement mean from the scal doc https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\nretailRDD.takeSample(false, 10, 1).foreach(println)","user":"anonymous","dateUpdated":"2019-09-25T17:24:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"retailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/miriam/datasets/online_retail/online-retail-dataset.txt MapPartitionsRDD[8] at textFile at <console>:25\nres25: Long = 541910\nfirstE: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n552290,21430,SET/3 RED GINGHAM ROSE STORAGE BOX,1,5/8/2011 13:32,3.75,16007,United Kingdom\n578349,22636,CHILDS BREAKFAST SET CIRCUS PARADE,2,11/24/2011 9:50,8.5,14539,United Kingdom\n537666,84917,WHITE HAND TOWEL WITH BUTTERFLY,1,12/7/2010 18:36,4.21,,United Kingdom\n547021,20749,ASSORTED COLOUR MINI CASES,2,3/18/2011 15:43,7.95,13046,United Kingdom\n553718,35809A,ENAMEL PINK TEA CONTAINER,1,5/18/2011 16:14,2.46,,United Kingdom\n557466,21242,RED RETROSPOT PLATE ,8,6/20/2011 13:08,1.69,13815,Germany\n567160,21218,RED SPOTTY BISCUIT TIN,1,9/18/2011 10:35,3.75,14562,United Kingdom\n548975,17003,BROCADE RING PURSE ,108,4/5/2011 11:47,0.29,17596,United Kingdom\n559338,85086A,CANDY SPOT HEART DECORATION,1,7/7/2011 16:30,0.83,,United Kingdom\n546769,22499,WOODEN UNION JACK BUNTING,3,3/16/2011 14:57,5.95,17504,United Kingdom\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=5","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=6","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=7","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=8"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403838_-67883841","id":"20190920-182724_1961848616","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:24:56+0000","dateFinished":"2019-09-25T17:24:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:202"},{"text":"%md\n","user":"anonymous","dateUpdated":"2019-09-25T17:24:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403839_1670044960","id":"20190922-220256_1973670371","dateCreated":"2019-09-23T13:13:23+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:203"},{"text":"%md\n### CSV format issue\n- Discuss why there are some lines that have more than 8 columns (hint: csv format)\n- Discuss some possible solutions","user":"anonymous","dateUpdated":"2019-09-25T17:25:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>CSV format issue</h3>\n<ul>\n  <li>Discuss why there are some lines that have more than 8 columns (hint: csv format)</li>\n  <li>Discuss some possible solutions</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403839_736128971","id":"20190921-023538_989684097","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:25:00+0000","dateFinished":"2019-09-25T17:25:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:204"},{"text":"%spark\n//Check CSV format\nval splitRdd = retailRDD.map(s => s.split(\",\"))\n\n//Some lines have more than 8 column which indicates a format issue\nval samples = splitRdd.map(arr => arr.length).takeSample(false,15, 11)\n\n//find out how many lines have more than 8 cols\nval lenArrRdd = splitRdd.map(arr => (arr.length, arr))\nlenArrRdd.filter(_._1 != 8).take(3).foreach({case(count, cols) => println(count + \":\" + cols.mkString(\"||\"))})\n","user":"anonymous","dateUpdated":"2019-09-25T17:25:00+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"splitRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[10] at map at <console>:29\nsamples: Array[Int] = Array(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8)\nlenArrRdd: org.apache.spark.rdd.RDD[(Int, Array[String])] = MapPartitionsRDD[13] at map at <console>:31\n9:536381||82567||\"AIRLINE LOUNGE||METAL SIGN\"||2||12/1/2010 9:41||2.1||15311||United Kingdom\n9:536394||21506||\"FANCY FONT BIRTHDAY CARD|| \"||24||12/1/2010 10:39||0.42||13408||United Kingdom\n9:536520||22760||\"TRAY|| BREAKFAST IN BED\"||1||12/1/2010 12:43||12.75||14729||United Kingdom\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=9","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=10","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=11"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403839_-221863145","id":"20190921-023311_1509488233","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:25:01+0000","dateFinished":"2019-09-25T17:25:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:205"},{"text":"%md\n### Pre-process dataset\n\nWe need to deal with fields that contain commas. e.g. `123,\"seond, field\",\"third field\"`. We have seen this csv format issue in Hive, and we solved it using `OpenCSV` SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.\n\n- Removing commas in `Description` field (e.g. \"Apple, Inc\" => \"Apple Inc\")<br>`awk -F'\"' -v OFS='' '{ for (i=2; i<=NF; i+=2) gsub(\",\", \"\", $i) } 1' online-retail-dataset.txt`\n- Remove all double double quotes<br>`sed 's/\"//g' online-retail-dataset.txt`\n- output file: `online-retail-dataset_clean.txt`\n\n### Move file to HDFS\nMove `online-retail-dataset_clean.txt` to GCP Dataproc HDFS\n\n### Spark RDD Cache\n- https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type","user":"anonymous","dateUpdated":"2019-09-25T17:25:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pre-process dataset</h3>\n<p>We need to deal with fields that contain commas. e.g. <code>123,&quot;seond, field&quot;,&quot;third field&quot;</code>. We have seen this csv format issue in Hive, and we solved it using <code>OpenCSV</code> SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.</p>\n<ul>\n  <li>Removing commas in <code>Description</code> field (e.g. &ldquo;Apple, Inc&rdquo; =&gt; &ldquo;Apple Inc&rdquo;)<br><code>awk -F&#39;&quot;&#39; -v OFS=&#39;&#39; &#39;{ for (i=2; i&lt;=NF; i+=2) gsub(&quot;,&quot;, &quot;&quot;, $i) } 1&#39; online-retail-dataset.txt</code></li>\n  <li>Remove all double double quotes<br><code>sed &#39;s/&quot;//g&#39; online-retail-dataset.txt</code></li>\n  <li>output file: <code>online-retail-dataset_clean.txt</code></li>\n</ul>\n<h3>Move file to HDFS</h3>\n<p>Move <code>online-retail-dataset_clean.txt</code> to GCP Dataproc HDFS</p>\n<h3>Spark RDD Cache</h3>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\">https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403840_989609156","id":"20190519-113048_765206384","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:25:04+0000","dateFinished":"2019-09-25T17:25:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:206"},{"text":"//Load csv file\n//Lazy evaluation\n//val datasetDir = \"/home/centos/dev/jrvs/bootcamp/hadoop/datasets\"\nval filePath = \"hdfs:///user/miriam/datasets/online_retail/online-retail-dataset_clean.txt\"\nval retailRDD = sc.textFile(filePath)\n\n//RDD action triggers evaluation (in this case count is an action)\nval count = retailRDD.count\n\n//tip: Use tab key to auto-complete\nval sample3 = retailRDD.takeSample(false, 3, 22)\n\n//Make sure every row has exactly 0 columns\nval longRow = retailRDD.filter(row => row.split(\",\").length != 8 ).count\n\n//Cache RDD since it will be accessed frequently\nretailRDD.cache","user":"anonymous","dateUpdated":"2019-09-25T17:25:04+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filePath: String = hdfs:///user/miriam/datasets/online_retail/online-retail-dataset_clean.txt\nretailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/miriam/datasets/online_retail/online-retail-dataset_clean.txt MapPartitionsRDD[16] at textFile at <console>:27\ncount: Long = 541909\nsample3: Array[String] = Array(569457,22429,ENAMEL MEASURING JUG CREAM,1,10/4/2011 11:29,4.25,14606,United Kingdom, 571265,22586,FELTCRAFT HAIRBAND PINK AND BLUE,5,10/16/2011 11:31,0.85,16674,United Kingdom, 563893,21244,BLUE POLKADOT PLATE ,2,8/19/2011 17:10,1.69,16330,United Kingdom)\nlongRow: Long = 0\nres42: retailRDD.type = hdfs:///user/miriam/datasets/online_retail/online-retail-dataset_clean.txt MapPartitionsRDD[16] at textFile at <console>:27\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=12","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=13","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=14","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=15"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403840_141318793","id":"20190519-105016_1691323616","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:25:05+0000","dateFinished":"2019-09-25T17:25:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:207"},{"text":"//Making some utilities and make your life easier :)\nval printRddNSamples = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.takeSample(false, n, 22).foreach(println)\nval printRdd3Samples = (rdd: org.apache.spark.rdd.RDD[_]) => printRddNSamples(rdd, 3)\nval printRddTopN = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.take(n).foreach(println)\nval bars = \"---------\"\nval printMsg = (msg:String) => println(bars+msg+bars)\n","user":"anonymous","dateUpdated":"2019-09-25T17:25:08+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"printRddNSamples: (org.apache.spark.rdd.RDD[_], Int) => Unit = <function2>\nprintRdd3Samples: org.apache.spark.rdd.RDD[_] => Unit = <function1>\nprintRddTopN: (org.apache.spark.rdd.RDD[_], Int) => Unit = <function2>\nbars: String = ---------\nprintMsg: String => Unit = <function1>\n"}]},"apps":[],"jobName":"paragraph_1569244403840_-1415128645","id":"20190519-192640_1954412488","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:25:09+0000","dateFinished":"2019-09-25T17:25:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:208"},{"text":"%md\n### Spark RDD Transormations and Actions\n\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a>\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a>\n- Databrick RDD operations http://bit.ly/30ez9IG\n- `SDG` chapter 12 & 13\n\n#### RDD Actions\n1. Get the first element from `retailRDD`\n2. Get the first 5 elements from `retailRDD` as an array.\n3. Get all elements from `retailRDD` as an array\n4. Get random 5 elements from `retailRDD` as an array\n5. Save all elements from `retailRDD` to local file `hdfs:///tmp/text.txt`\n\nSample outputs:\n```bash\n#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n```\n\n","user":"anonymous","dateUpdated":"2019-09-25T17:25:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Spark RDD Transormations and Actions</h3>\n<ul>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a></li>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a></li>\n  <li>Databrick RDD operations <a href=\"http://bit.ly/30ez9IG\">http://bit.ly/30ez9IG</a></li>\n  <li><code>SDG</code> chapter 12 &amp; 13</li>\n</ul>\n<h4>RDD Actions</h4>\n<ol>\n  <li>Get the first element from <code>retailRDD</code></li>\n  <li>Get the first 5 elements from <code>retailRDD</code> as an array.</li>\n  <li>Get all elements from <code>retailRDD</code> as an array</li>\n  <li>Get random 5 elements from <code>retailRDD</code> as an array</li>\n  <li>Save all elements from <code>retailRDD</code> to local file <code>hdfs:///tmp/text.txt</code></li>\n</ol>\n<p>Sample outputs:</p>\n<pre><code class=\"bash\">#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403840_1946141232","id":"20190519-115905_2023471169","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:25:11+0000","dateFinished":"2019-09-25T17:25:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:209"},{"text":"printMsg(\"#1\")\nretailRDD.first\n\nprintMsg(\"#2\")\nretailRDD.take(5)\n\nprintMsg(\"#3\")\nretailRDD.collect()\n\nprintMsg(\"#4\")\nretailRDD.takeSample(false, 5, 34)\n\nprintMsg(\"#5\")\nretailRDD.saveAsTextFile(\"hdfs:///tmp/text.txt\")","user":"anonymous","dateUpdated":"2019-09-25T17:25:11+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":402.246,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"---------#1---------\nres47: String = 536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n---------#2---------\nres49: Array[String] = Array(536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom, 536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom, 536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom, 536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom, 536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom)\n---------#3---------\nres51: Array[String] = Array(536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom, 536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom, 536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom, 536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom, 536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom, 536365,22752,SET 7 BABUSHKA NESTING BOXES,2,12/1/2010 8:26,7.65,17850,United Kingdom, 536365,21730,GLASS STAR FROSTED T-LIGHT HOLDER,6,12/1/2010 8:26,4.25,17850,United Kingdom, 536366,22633,HAND WARMER UNION JACK,6,12/1/2010 8:28,1.85,17850,United Kingdom, 536366,22632,HAND WARMER RED POLKA DOT,6,12/1/2010 8:28,1.85,17850,Un...---------#4---------\nres53: Array[String] = Array(537362,22097,SWALLOW SQUARE TISSUE BOX,2,12/6/2010 12:34,1.25,17596,United Kingdom, 557308,23300,GARDENERS KNEELING PAD CUP OF TEA ,1,6/19/2011 15:13,1.65,16744,United Kingdom, 574863,23456,MEDIUM PARLOUR PICTURE FRAME ,2,11/7/2011 12:29,4.15,15719,United Kingdom, 578067,21494,ROTATING LEAVES T-LIGHT HOLDER,7,11/22/2011 15:43,2.46,,United Kingdom, 575612,20719,WOODLAND CHARLOTTE BAG,10,11/10/2011 12:47,0.85,17416,United Kingdom)\n---------#5---------\norg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory hdfs://jarvis-bootcamp-m/tmp/text.txt already exists\n  at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:287)\n  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1096)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1094)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1067)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1032)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:958)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:957)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1499)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1478)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1478)\n  ... 52 elided\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=16","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=17","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=18","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=19","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=20"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403841_1177982223","id":"20190519-122034_629713430","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:25:11+0000","dateFinished":"2019-09-25T17:25:17+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:210"},{"text":"%md","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403841_-413635130","id":"20190922-215221_1578966852","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:211"},{"text":"%md\n#### RDD Transformations\nRDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. `printRddNSamples` uses `takeSample` action) \n\n1. Get all sales from \"United Kingdom\" (hint: use filter)\n2. Compare `sample` and `takeSample`\n\nSampel outputs:\n```bash\n#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n```","user":"anonymous","dateUpdated":"2019-09-24T13:24:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD Transformations</h4>\n<p>RDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. <code>printRddNSamples</code> uses <code>takeSample</code> action) </p>\n<ol>\n  <li>Get all sales from &ldquo;United Kingdom&rdquo; (hint: use filter)</li>\n  <li>Compare <code>sample</code> and <code>takeSample</code></li>\n</ol>\n<p>Sampel outputs:</p>\n<pre><code class=\"bash\">#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403841_-1286628501","id":"20190917-181850_863623231","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-24T13:24:57+0000","dateFinished":"2019-09-24T13:24:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:212"},{"text":"printMsg(\"#1\")\nval countUK = retailRDD.filter(line => line.contains(\"United Kingdom\")).count\n\nprintMsg(\"#2\")\nprintln(\"takeSmaple returns a fixed-size sample subset in an array\")\nprintln(\"sample returns a sample subset that is approximately of size sizeOfRDD * fraction\")\nretailRDD.sample(false, 0.001, 10).count\nretailRDD.count * 0.001","user":"anonymous","dateUpdated":"2019-09-24T13:30:40+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------#1---------\ncountUK: Long = 495478\n---------#2---------\ntakeSmaple returns a fixed-size sample subset in an array\nsample returns a sample subset that is approximately of size sizeOfRDD * fraction\nres46: Long = 537\nres47: Double = 541.909\n"}]},"apps":[],"jobName":"paragraph_1569244403842_158541210","id":"20190519-124053_1683164197","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-24T13:30:40+0000","dateFinished":"2019-09-24T13:30:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:213"},{"text":"%md\n### Pair RDD (KeyValue)\nSo far, each element in `retailRDD` is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let's construct Pair RDDs from `retailRDD` in order to perform more advanced computations.\n\n**RDD vs PairRDD**:\n\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pair RDD (KeyValue)</h3>\n<p>So far, each element in <code>retailRDD</code> is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let&rsquo;s construct Pair RDDs from <code>retailRDD</code> in order to perform more advanced computations.</p>\n<p><strong>RDD vs PairRDD</strong>:</p>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403842_164291794","id":"20190519-125017_38292448","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:214"},{"text":"%md\n\n#### Questions 1.0\n\nTrnasform each element in `retailRDD` to a key value pair (as a tuple) as following\n\n```\nkey = country\nvalue = amount (e.g. Quantity * UnitPrice)\n```\n\nhint: \n\n- use `rdd.map`\n- Use `row.split(\",\")` to tokenize the row\n- Cast quanitity to int while parsing the row\n- Cast price to double\n\n**Sample output**:\n\n```\n//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n```\n\n","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.0</h4>\n<p>Trnasform each element in <code>retailRDD</code> to a key value pair (as a tuple) as following</p>\n<pre><code>key = country\nvalue = amount (e.g. Quantity * UnitPrice)\n</code></pre>\n<p>hint: </p>\n<ul>\n  <li>use <code>rdd.map</code></li>\n  <li>Use <code>row.split(&quot;,&quot;)</code> to tokenize the row</li>\n  <li>Cast quanitity to int while parsing the row</li>\n  <li>Cast price to double</li>\n</ul>\n<p><strong>Sample output</strong>:</p>\n<pre><code>//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403843_242820983","id":"20190519-195132_1947538683","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:215"},{"text":"%spark\n//Qustion 1.0 solution\nval parseKeyValue = (row: String ) => {\n    val tokens = row.split(\",\")\n    val country= tokens.last\n    val quantity = tokens(3)\n    val unitPrice = tokens(5)\n    val amount = quantity.toInt * unitPrice.toDouble\n    (country, amount)\n}\n\n\nval ctryRdd = retailRDD.map(parseKeyValue)\nctryRdd.takeSample(false, 3,3)","user":"anonymous","dateUpdated":"2019-09-24T13:30:44+0000","config":{"lineNumbers":false,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":822,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"parseKeyValue: String => (String, Double) = <function1>\nctryRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[21] at map at <console>:31\nres50: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n"}]},"apps":[],"jobName":"paragraph_1569244403843_-2099182974","id":"20190519-125921_348001552","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-24T13:30:44+0000","dateFinished":"2019-09-24T13:30:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:216"},{"user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403843_57922497","id":"20190922-215540_2030793994","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:217"},{"text":"%md\n\n#### Questions 1.1\n\nCalculate total sales amount for each country and sort in descending order\n\n```sql\nSELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n```\n\n**Sample output**\n\n```bash\n//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n```\n\nHints:\n\n- implement `group by` with `rdd.reduceByKey` [doc](http://bit.ly/30fJHHs)\n- implement `order by` with `rdd.sortBy`","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.1</h4>\n<p>Calculate total sales amount for each country and sort in descending order</p>\n<pre><code class=\"sql\">SELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n</code></pre>\n<p>Hints:</p>\n<ul>\n  <li>implement <code>group by</code> with <code>rdd.reduceByKey</code> <a href=\"http://bit.ly/30fJHHs\">doc</a></li>\n  <li>implement <code>order by</code> with <code>rdd.sortBy</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403844_2053101544","id":"20190519-195238_1235609517","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:218"},{"text":"%spark\nprintMsg(\"(Country, total sales)\")\nval salesByCountry = ctryRdd.reduceByKey((a,b) => a + b)\nsalesByCountry.take(3)\n\nprintMsg(\"order\")\nval salesByCountryOrdered = salesByCountry.sortBy(v => v._2, false)\n//false ensures descending ordering otherwise it would order ascending\nsalesByCountryOrdered.take(3)","user":"anonymous","dateUpdated":"2019-09-24T13:30:46+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------(Country, total sales)---------\nsalesByCountry: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[23] at reduceByKey at <console>:33\nres53: Array[(String, Double)] = Array((Australia,137077.2699999999), (Portugal,29367.019999999953), (United Kingdom,8187806.363998696))\n---------order---------\nsalesByCountryOrdered: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[28] at sortBy at <console>:35\nres56: Array[(String, Double)] = Array((United Kingdom,8187806.363998696), (Netherlands,284661.539999999), (EIRE,263276.81999999884))\n"}]},"apps":[],"jobName":"paragraph_1569244403844_-1545040367","id":"20190519-195236_1582695577","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-24T13:30:46+0000","dateFinished":"2019-09-24T13:30:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:219"},{"user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403844_-1629556973","id":"20190922-215553_1555963294","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:220"},{"text":"%md\n\n#### Questions 2.0\n\nLet's assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)\n\n```sql\nSELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n```\n\n**Sample output**\n\n```\n//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n```\n\n**hints:**\n\n- Generate a new KV pair RDD `(country, id)`\n- use `rdd.reduceByKey` find the smallest\n- ID must be a numric number\n","user":"anonymous","dateUpdated":"2019-09-24T13:58:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 2.0</h4>\n<p>Let&rsquo;s assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)</p>\n<pre><code class=\"sql\">SELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code>//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n</code></pre>\n<p><strong>hints:</strong></p>\n<ul>\n  <li>Generate a new KV pair RDD <code>(country, id)</code></li>\n  <li>use <code>rdd.reduceByKey</code> find the smallest</li>\n  <li>ID must be a numric number</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403844_627303071","id":"20190519-195157_1405617071","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-24T13:58:43+0000","dateFinished":"2019-09-24T13:58:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:221"},{"text":"%spark\nprintMsg(\"generate kv pair\")\nval parseCountryId = (row: String ) => {\n    val tokens = row.split(\",\")\n    val country= tokens.last\n    val id = tokens(6)\n    val idInt = {if(id == \"\") Int.MaxValue else id.toInt}\n    (country, idInt)\n}\nval countryIdRDD = retailRDD.map(parseCountryId)\ncountryIdRDD.take(3)\n\nprintMsg(\"Result\")\nval countrySmallestIdRDD = countryIdRDD.reduceByKey((x,y) => scala.math.min(x,y))\ncountrySmallestIdRDD.collect","user":"anonymous","dateUpdated":"2019-09-24T13:58:20+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------generate kv pair---------\nparseCountryId: String => (String, Int) = <function1>\ncountryIdRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[41] at map at <console>:31\nres95: Array[(String, Int)] = Array((United Kingdom,17850), (United Kingdom,17850), (United Kingdom,17850))\n---------Result---------\ncountrySmallestIdRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[42] at reduceByKey at <console>:33\nres97: Array[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n"}]},"apps":[],"jobName":"paragraph_1569244403845_444909305","id":"20190519-144439_1578143376","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-24T13:58:20+0000","dateFinished":"2019-09-24T13:58:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:222"},{"user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403845_1790335786","id":"20190922-215609_1113478409","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:223"},{"text":"%md\n\n### Question 2.1\n\nIt's inconvenient to tokenize each row in every operation. Instead, we count convert `retailRDD[String]` to `itemsRdd:RDD[Item]` where `Item` is a case class as following:\n\n`case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)`\n\nIn this way, you only parse each row only once here and you can reuse `itemsRdd` in the rest of the questions.\n\n**Sample outputs**\n```scala\n//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n```\n\n**Hints**:\n\n- write a function to convert a row to a Item, e.g. \n```\nval parseRow2Item: (String) => Item = (row: String) => {\n    //complete body\n}\n```\n- Covert all rows to items, e.g. `val itemsRdd =  retailRDD.map(parseRow2Item)`\n","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Question 2.1</h3>\n<p>It&rsquo;s inconvenient to tokenize each row in every operation. Instead, we count convert <code>retailRDD[String]</code> to <code>itemsRdd:RDD[Item]</code> where <code>Item</code> is a case class as following:</p>\n<p><code>case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)</code></p>\n<p>In this way, you only parse each row only once here and you can reuse <code>itemsRdd</code> in the rest of the questions.</p>\n<p><strong>Sample outputs</strong></p>\n<pre><code class=\"scala\">//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n</code></pre>\n<p><strong>Hints</strong>:</p>\n<ul>\n  <li>\n  <p>write a function to convert a row to a Item, e.g. </p>\n  <pre><code>val parseRow2Item: (String) =&gt; Item = (row: String) =&gt; {\n//complete body\n}\n</code></pre></li>\n  <li>Covert all rows to items, e.g. <code>val itemsRdd =  retailRDD.map(parseRow2Item)</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403845_46688347","id":"20190921-180308_1963749922","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224"},{"text":"%spark\ncase class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], country:String)\n\nval parseRowToItem = (row:String) => {\n    val tokens = row.split(\",\")\n    val invoiceNo = tokens(0)\n    val stockCode = tokens(1)\n    val description = if(tokens(2) == \"\") None else Some(tokens(2))\n    val quantity = tokens(3).toInt\n    val invoiceDate = tokens(4)\n    val unitPrice = tokens(5).toDouble\n    val customerId = if(tokens(6) == \"\") None else Some(tokens(6).toInt)\n    val country = tokens.last\n    Item(invoiceNo, stockCode, description, quantity, invoiceDate, unitPrice, customerId, country)\n}\n\nval itemsRDD = retailRDD.map(parseRowToItem)\nitemsRDD.take(3)","user":"anonymous","dateUpdated":"2019-09-25T17:25:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Item\nparseRowToItem: String => Item = <function1>\nitemsRDD: org.apache.spark.rdd.RDD[Item] = MapPartitionsRDD[22] at map at <console>:33\nres58: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=21"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403846_-1514038504","id":"20190921-180433_1776305327","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:25:41+0000","dateFinished":"2019-09-25T17:25:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:225"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403846_2017870749","id":"20190922-215634_337314214","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:226"},{"text":"%md\n### Questions 2.2\n\nRe-implement questions 1.1 & 2.1 using itemsRdd (`RDD[Item]`)","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Questions 2.2</h3>\n<p>Re-implement questions 1.1 &amp; 2.1 using itemsRdd (<code>RDD[Item]</code>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403846_-1408773702","id":"20190922-140917_1244358721","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:227"},{"text":"%spark\nprintMsg(\"Re-implement question 1.1.\")\nitemsRDD.map(item => (item.country, item.quantity * item.unitPrice))\n\nprintMsg(\"Re-implement question 2.0\")\nitemsRDD.filter(item => item.customerID != None).map(item => (item.country, item.customerID.get)).reduceByKey((x,y) => scala.math.min(x,y)).collect","user":"anonymous","dateUpdated":"2019-09-25T17:44:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------Re-implement question 1.1.---------\nres151: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[131] at map at <console>:36\n---------Re-implement question 2.0---------\nres153: Array[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=44"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403847_-628980102","id":"20190922-141025_1178356031","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:44:45+0000","dateFinished":"2019-09-25T17:44:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:228"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403847_995101326","id":"20190922-215642_1393968112","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229"},{"text":"%md\n#### Questions 3\n\nFind number of customers.\n\n```\nSELECT distinct(customerId)\nFROM retail\n```\n\n**Sample output**\n```scala\n//resultRdd.count\nres458: Long = 4373\n```","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 3</h4>\n<p>Find number of customers.</p>\n<pre><code>SELECT distinct(customerId)\nFROM retail\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//resultRdd.count\nres458: Long = 4373\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403848_-989383384","id":"20190519-195407_556558321","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230"},{"text":"%spark\nval resultRDD = itemsRDD.map(item => item.customerID).distinct()\nresultRDD.count","user":"anonymous","dateUpdated":"2019-09-25T17:32:55+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"resultRDD: org.apache.spark.rdd.RDD[Option[Int]] = MapPartitionsRDD[115] at distinct at <console>:35\nres106: Long = 4373\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=41"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403848_339340223","id":"20190519-194831_1486531342","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:32:55+0000","dateFinished":"2019-09-25T17:32:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:231"},{"user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569244403848_251186114","id":"20190922-215651_1847295545","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:232"},{"text":"%md\n#### Question 4\n\nFind out the number of invoices/purchases for each customer.\n> Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)\n\n```sql\nSELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n```\n\n**Sample output**\n```scala\n//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n```\n","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Question 4</h4>\n<p>Find out the number of invoices/purchases for each customer.</p>\n<blockquote>\n  <p>Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)</p>\n</blockquote>\n<pre><code class=\"sql\">SELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403848_-1805446028","id":"20190519-195310_661372203","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:233"},{"text":"%spark\nval idInvRDD = itemsRDD.map(item => (item.customerID, item.invoiceNo))\nval distinctCustomerInvoiceRDD = idInvRDD.distinct()\ndistinctCustomerInvoiceRDD.count\nval idPurchasesRDD = distinctCustomerInvoiceRDD.filter({case(id,invoice) => id != None}).map({case(id, invoice) => (id.get,1)}).reduceByKey(_+_)\nprintRdd3Samples(idPurchasesRDD)","user":"anonymous","dateUpdated":"2019-09-25T17:32:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"idInvRDD: org.apache.spark.rdd.RDD[(Option[Int], String)] = MapPartitionsRDD[104] at map at <console>:35\ndistinctCustomerInvoiceRDD: org.apache.spark.rdd.RDD[(Option[Int], String)] = MapPartitionsRDD[107] at distinct at <console>:37\nres103: Long = 25900\nidPurchasesRDD: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[110] at reduceByKey at <console>:39\n(16893,1)\n(14051,26)\n(17881,1)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=38","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=39","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=40"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403852_-1056665847","id":"20190922-190215_1478690977","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:32:49+0000","dateFinished":"2019-09-25T17:32:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:234"},{"text":"%md\n#### SPARK UI\nRun previous paragrah and then use `Spark UI` to inspect Spark jobs \n(click the `SPAKR JOBS` buttom on the top right of the paragraph)","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>SPARK UI</h4>\n<p>Run previous paragrah and then use <code>Spark UI</code> to inspect Spark jobs<br/>(click the <code>SPAKR JOBS</code> buttom on the top right of the paragraph)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403852_-922021523","id":"20190521-114127_1095254606","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:235"},{"text":"%md\n#### RDD join\n\nPrint `customerId, name, country` using `customers.txt` and  `online-retail-dataset_clean.txt` files\n\n1. Load `datasets/online_retail/customers.txt` to `RDD[Customer]` where `Customer` is a case class as following\n`case class Customer(customerId:Int, name: String)`\n2. Join `RDD[Customer]` with `RDD[item]` on `customerId`\n3. Select uniq `customerId, name, country`\n\n```sql\nSELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n```\n\n**Sample Output**\n```bash\n//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n```","user":"anonymous","dateUpdated":"2019-09-23T13:13:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD join</h4>\n<p>Print <code>customerId, name, country</code> using <code>customers.txt</code> and <code>online-retail-dataset_clean.txt</code> files</p>\n<ol>\n  <li>Load <code>datasets/online_retail/customers.txt</code> to <code>RDD[Customer]</code> where <code>Customer</code> is a case class as following<br/><code>case class Customer(customerId:Int, name: String)</code></li>\n  <li>Join <code>RDD[Customer]</code> with <code>RDD[item]</code> on <code>customerId</code></li>\n  <li>Select uniq <code>customerId, name, country</code></li>\n</ol>\n<pre><code class=\"sql\">SELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n</code></pre>\n<p><strong>Sample Output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569244403853_1776140020","id":"20190519-183851_617743118","dateCreated":"2019-09-23T13:13:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236"},{"text":"%spark\n\ncase class Customer(customerId:Int, name: String)\n\nval customerRDD = sc.textFile(\"hdfs:///user/miriam/datasets/online_retail/customers.txt\").map(row => {\n    val tokens = row.split(\",\")\n    Customer(tokens(0).toInt, tokens(1).trim)\n})\nprintRddTopN(customerRDD, 4)","user":"anonymous","dateUpdated":"2019-09-25T17:29:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Customer\ncustomerRDD: org.apache.spark.rdd.RDD[Customer] = MapPartitionsRDD[76] at map at <console>:28\nCustomer(15930,Philip V. Bradford)\nCustomer(17796,Alvin V. Ellison)\nCustomer(15550,Nero D. Walls)\nCustomer(17934,Uma S. Stephens)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=24"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403853_1518289716","id":"20190922-193513_1093352183","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:30:00+0000","dateFinished":"2019-09-25T17:30:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:237"},{"text":"%spark\nprintMsg(\"key=customerId, value = customer\")\nval customerKvRDD = customerRDD.map(customer => (customer.customerId, customer))\n\nprintMsg(\"key=customerID, value = item\")\nval itemsKvRDD = itemsRDD.filter(item => item.customerID != None).map(item => (item.customerID.get, item))\n\nprintMsg(\"join\")\nval joinedRDD = itemsKvRDD.leftOuterJoin(customerKvRDD)\n\nprintMsg(\"result\")\nval resultRDD = joinedRDD.filter({case(id,(item, customer)) => customer != None}).map({case(id, (item, customer)) =>(id, customer.get.name, item.country)}).distinct()\nprintRddTopN(resultRDD, 3)","user":"anonymous","dateUpdated":"2019-09-25T17:29:23+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":325.994,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------key=customerId, value = customer---------\ncustomerKvRDD: org.apache.spark.rdd.RDD[(Int, Customer)] = MapPartitionsRDD[63] at map at <console>:29\n---------key=customerID, value = item---------\nitemsKvRDD: org.apache.spark.rdd.RDD[(Int, Item)] = MapPartitionsRDD[65] at map at <console>:35\n---------join---------\njoinedRDD: org.apache.spark.rdd.RDD[(Int, (Item, Option[Customer]))] = MapPartitionsRDD[68] at leftOuterJoin at <console>:43\n---------result---------\nresultRDD: org.apache.spark.rdd.RDD[(Int, String, String)] = MapPartitionsRDD[73] at distinct at <console>:45\n(13311,Petra M. Dalton,United Kingdom)\n(16852,Rama R. Savage,United Kingdom)\n(18144,Gage A. Sharpe,United Kingdom)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=23"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569244403853_-168244132","id":"20190922-193954_207087197","dateCreated":"2019-09-23T13:13:23+0000","dateStarted":"2019-09-25T17:29:23+0000","dateFinished":"2019-09-25T17:29:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:238"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2019-09-24T19:56:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569355014585_393056512","id":"20190924-195654_217702976","dateCreated":"2019-09-24T19:56:54+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:239"}],"name":"Spark project","id":"2ERAQQ9VZ","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}