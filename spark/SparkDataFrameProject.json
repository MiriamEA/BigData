{"paragraphs":[{"text":"%md\n## Learning Dataset/DataFrame\nReadings: \n\n- `Spark - The Definitive Guide` chapter 3 - 10\n- The official document https://spark.apache.org/docs/2.3.3/sql-programming-guide.html","user":"anonymous","dateUpdated":"2019-10-17T17:15:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Learning Dataset/DataFrame</h2>\n<p>Readings: </p>\n<ul>\n  <li><code>Spark - The Definitive Guide</code> chapter 3 - 10</li>\n  <li>The official document <a href=\"https://spark.apache.org/docs/2.3.3/sql-programming-guide.html\">https://spark.apache.org/docs/2.3.3/sql-programming-guide.html</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605414_-781537571","id":"20191007-143023_835288946","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:15:29+0000","dateFinished":"2019-10-17T17:15:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:237"},{"text":"%md\n### Creating Dataframes\n#### Creating Dataframes from Scala `Seq`\n\nWe can convert a Sequence of Tuples to a Spark DF.\ne.g. Seq[(String, Double, String, String)] \n\nA tuple corresponds to a DF row.\nA element in a tuple corresponds to a column to a particular row.","user":"anonymous","dateUpdated":"2019-10-17T17:15:31+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating Dataframes</h3>\n<h4>Creating Dataframes from Scala <code>Seq</code></h4>\n<p>We can convert a Sequence of Tuples to a Spark DF.<br/>e.g. Seq[(String, Double, String, String)] </p>\n<p>A tuple corresponds to a DF row.<br/>A element in a tuple corresponds to a column to a particular row.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605433_-1087389260","id":"20190519-201210_1157722001","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:15:32+0000","dateFinished":"2019-10-17T17:15:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:238"},{"text":"%md\n#### Scala Implicit Conversions\n\nIn short, you have to `import spark.implicits._` to convert/cast a `Seq[(String, Double, String, String)]` to a Spark `DataFrame`. (e.g. `lineTupleSeq.toDF`)\n\n#####  (Advanced)\nThis is called implicit conversions in Scala. In this case, `spark.implicits.localSeqToDatasetHolder` creates a Dataset from a local Seq.\n\nSpark Scala Docs:\n- <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a>\n- <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a>","user":"anonymous","dateUpdated":"2019-10-17T17:15:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Scala Implicit Conversions</h4>\n<p>In short, you have to <code>import spark.implicits._</code> to convert/cast a <code>Seq[(String, Double, String, String)]</code> to a Spark <code>DataFrame</code>. (e.g. <code>lineTupleSeq.toDF</code>)</p>\n<h5>(Advanced)</h5>\n<p>This is called implicit conversions in Scala. In this case, <code>spark.implicits.localSeqToDatasetHolder</code> creates a Dataset from a local Seq.</p>\n<p>Spark Scala Docs:<br/>- <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a><br/>- <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605442_1902954520","id":"20190520-102917_1809142825","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:15:32+0000","dateFinished":"2019-10-17T17:15:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:239"},{"text":"/**\n * Each line/record/row must be a Tuple\n * e.g.  Tuple(AAPL,110.5,2018-02-01,Apple)\n * \n * Lines are grouped into a Seq\n * List(\n *   (AAPL,110.5,2018-02-01,Apple),\n *   (AMZN,1500.52,2018-02-01,Ammazon.com),\n *   (FB,170.01,2018-02-01,Facebook)\n * )\n */\nval lineTuple1 = (\"AAPL\",110.5,\"2018-02-01\",\"Apple\")\nval lineTuple2 = (\"AMZN\",1500.52,\"2018-02-01\",\"Ammazon.com\")\nval lineTuple3 = (\"FB\",170.01,\"2018-02-01\",\"Facebook\")\nval lineTupleSeq = Seq(lineTuple1,lineTuple2,lineTuple3)\n\n//To use toDF, you must import this (see next section for details)\n//In fact Zeppellin interpreter already imported this for you\nimport spark.implicits._\nval stockDf = lineTupleSeq.toDF(\"ticker\",\"price\", \"date\", \"companyName\")\nstockDf.printSchema\n\n//SELECT * FROM stock LIMIT 3\nstockDf.show(3)\n\n//SELECT companyName AS company_name, price FROM stock\nstockDf.select(col(\"companyName\").as(\"company_name\"), col(\"price\")).show()","user":"anonymous","dateUpdated":"2019-10-17T17:15:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lineTuple1: (String, Double, String, String) = (AAPL,110.5,2018-02-01,Apple)\nlineTuple2: (String, Double, String, String) = (AMZN,1500.52,2018-02-01,Ammazon.com)\nlineTuple3: (String, Double, String, String) = (FB,170.01,2018-02-01,Facebook)\nlineTupleSeq: Seq[(String, Double, String, String)] = List((AAPL,110.5,2018-02-01,Apple), (AMZN,1500.52,2018-02-01,Ammazon.com), (FB,170.01,2018-02-01,Facebook))\nimport spark.implicits._\nstockDf: org.apache.spark.sql.DataFrame = [ticker: string, price: double ... 2 more fields]\nroot\n |-- ticker: string (nullable = true)\n |-- price: double (nullable = false)\n |-- date: string (nullable = true)\n |-- companyName: string (nullable = true)\n\n+------+-------+----------+-----------+\n|ticker|  price|      date|companyName|\n+------+-------+----------+-----------+\n|  AAPL|  110.5|2018-02-01|      Apple|\n|  AMZN|1500.52|2018-02-01|Ammazon.com|\n|    FB| 170.01|2018-02-01|   Facebook|\n+------+-------+----------+-----------+\n\n+------------+-------+\n|company_name|  price|\n+------------+-------+\n|       Apple|  110.5|\n| Ammazon.com|1500.52|\n|    Facebook| 170.01|\n+------------+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1570548605451_-439183077","id":"20190519-201416_412351679","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:15:33+0000","dateFinished":"2019-10-17T17:16:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:240"},{"text":"%md\n### Creating DF from CSV Files","user":"anonymous","dateUpdated":"2019-10-17T17:16:18+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating DF from CSV Files</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605452_-1279376681","id":"20190520-104920_1833330750","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:16:19+0000","dateFinished":"2019-10-17T17:16:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:241"},{"text":"//Read CSV file to df\n//local or hdfs path\nval path = \"hdfs:///user/miriam/datasets/online_retail/online-retail-dataset.txt\"\n\n//spark.read is able to handle csv formats\nval retailDf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(path)\n\nretailDf.printSchema\nretailDf.show(3)\nretailDf.show(3,false)","user":"anonymous","dateUpdated":"2019-10-17T17:16:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"path: String = hdfs:///user/miriam/datasets/online_retail/online-retail-dataset.txt\nretailDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN               |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=0","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=1","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=2","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=3"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605452_504936342","id":"20190520-095229_630927102","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:16:19+0000","dateFinished":"2019-10-17T17:16:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:242"},{"text":"%md\n#### Column Type Cast\nIn `retailDf` schema, `InvoiceDate` column data type is string. \n\nIn this practice, you need to cast `InvoiceDate` column to a Spark `timestamp` data type\n\n```bash\nresultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n```\n","user":"anonymous","dateUpdated":"2019-10-17T17:16:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Column Type Cast</h4>\n<p>In <code>retailDf</code> schema, <code>InvoiceDate</code> column data type is string. </p>\n<p>In this practice, you need to cast <code>InvoiceDate</code> column to a Spark <code>timestamp</code> data type</p>\n<pre><code class=\"bash\">resultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605453_1730239428","id":"20190520-085947_2007764287","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:16:38+0000","dateFinished":"2019-10-17T17:16:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:243"},{"text":"import org.apache.spark.sql.functions.to_timestamp\nval format = \"MM/dd/yyyy HH:mm\"\nval retailCastDf = retailDf.select(col(\"InvoiceNo\"), col(\"StockCode\"), col(\"Description\"), col(\"Quantity\"), to_timestamp(col(\"InvoiceDate\"), format) as \"InvoiceDate\", col(\"UnitPrice\"), col(\"CustomerID\"), col(\"Country\"))\nretailCastDf.printSchema\n//Cache DF in memory since it will be accessed frequently\nretailCastDf.cache","user":"anonymous","dateUpdated":"2019-10-17T17:16:38+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions.to_timestamp\nformat: String = MM/dd/yyyy HH:mm\nretailCastDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\nres22: retailCastDf.type = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"}]},"apps":[],"jobName":"paragraph_1570548605454_1484382579","id":"20190519-215300_721200493","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:16:39+0000","dateFinished":"2019-10-17T17:16:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:244"},{"text":"%md\n### DataFrame SELECT\nImplement the following SQL queries using dataframe. Compare different select syntax.\n\n```sql\nSELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n```","user":"anonymous","dateUpdated":"2019-10-17T17:16:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>DataFrame SELECT</h3>\n<p>Implement the following SQL queries using dataframe. Compare different select syntax.</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605454_1201558018","id":"20190519-221054_1925024171","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:16:41+0000","dateFinished":"2019-10-17T17:16:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:245"},{"text":"//SELECT * from retail limit 1;\nretailCastDf.show(1)\nimport org.apache.spark.sql.functions._\n//select InvoiceNo,CustomerID,Country from retail limit 1;\nretailCastDf.select(\"InvoiceNo\").show(1)\n\n//Different ways of select \nretailCastDf.select($\"InvoiceNo\").show(1)\nretailCastDf.select('InvoiceNo).show(1)\nretailCastDf.select(col(\"InvoiceNo\")).show(1)\nretailCastDf.select(retailCastDf.col(\"InvoiceNo\")).show(1)\nretailCastDf.select(expr(\"InvoiceNo\")).show(1)\n\n//ERROR: cannot mix \n//retailCastDf.select($\"InvoiceNo\", \"StockCode\").show(1)\n\n//expr or selectExpr is most powerful and close to SQL syntax\n//SELECT InvoiceNo as invoiceId from retail limit 1;\nretailCastDf.select(expr(\"InvoiceNo as invoiceId\")).show(1)\nretailCastDf.selectExpr(\"InvoiceNo as invoiceId\").show(1)\n\n//SELECT * from retail limit 1;\nretailCastDf.selectExpr(\"*\").show(1)\n\n//select max(UnitPrice) as maxUnitPrice from retail\nretailCastDf.selectExpr(\"max(UnitPrice) as maxUnitPrice\").show \n","user":"anonymous","dateUpdated":"2019-10-17T17:16:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 1 row\n\nimport org.apache.spark.sql.functions._\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|invoiceId|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|invoiceId|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 1 row\n\n+------------+\n|maxUnitPrice|\n+------------+\n|     38970.0|\n+------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=4","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=5","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=6","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=7","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=8","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=9","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=10","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=11","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=12","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=13","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=14"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605455_119222814","id":"20190519-211701_1956303781","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:16:41+0000","dateFinished":"2019-10-17T17:16:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:246"},{"text":"%md\n### DataFrame filtering (WHERE)\n\nImplement the following SQL quries\n\n```sql\nSELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n```\n\nSample results\n```\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n```","user":"anonymous","dateUpdated":"2019-10-17T17:16:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>DataFrame filtering (WHERE)</h3>\n<p>Implement the following SQL quries</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n</code></pre>\n<p>Sample results</p>\n<pre><code>+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605456_159324715","id":"20190519-221114_648626738","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:16:58+0000","dateFinished":"2019-10-17T17:16:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:247"},{"text":"retailCastDf.where(\"InvoiceNo = 536365\").show(2)","user":"anonymous","dateUpdated":"2019-10-17T17:16:58+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=15"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605457_-921733556","id":"20190519-201625_2028882244","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:16:58+0000","dateFinished":"2019-10-17T17:16:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:248"},{"user":"anonymous","dateUpdated":"2019-10-17T17:16:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1570548605467_-1526473012","id":"20191007-145852_244125478","dateCreated":"2019-10-08T15:30:05+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:249"},{"text":"%md\n\n### DF Exercises\n#### Spark SQL temp view\nFor the following DF exercises, instead of jumping right into DF solutions, you can write `sql` solutions and verify with Spark SQL Temp Views.","user":"anonymous","dateUpdated":"2019-10-17T17:16:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>DF Exercises</h3>\n<h4>Spark SQL temp view</h4>\n<p>For the following DF exercises, instead of jumping right into DF solutions, you can write <code>sql</code> solutions and verify with Spark SQL Temp Views.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605468_1119430069","id":"20190520-123428_698724288","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:17:00+0000","dateFinished":"2019-10-17T17:17:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:250"},{"text":"retailCastDf.createOrReplaceTempView(\"retailView\")\n\nspark.sql(\"SELECT * FROM retailView LIMIT 6\").show()","user":"anonymous","dateUpdated":"2019-10-17T17:17:00+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   563016|    21544|SKULLS  WATER TRA...|      12|2011-08-11 12:44:00|     0.85|     15358|United Kingdom|\n|   563016|    22931|BAKING MOULD HEAR...|       6|2011-08-11 12:44:00|     2.55|     15358|United Kingdom|\n|   563016|    22585|PACK OF 6 BIRDY G...|      12|2011-08-11 12:44:00|     1.25|     15358|United Kingdom|\n|   563016|    21497|FANCY FONTS BIRTH...|      25|2011-08-11 12:44:00|     0.42|     15358|United Kingdom|\n|   563016|    23191|BUNDLE OF 3 RETRO...|      12|2011-08-11 12:44:00|     1.65|     15358|United Kingdom|\n|   563016|    23167|SMALL CERAMIC TOP...|      12|2011-08-11 12:44:00|     0.83|     15358|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=16"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605469_-2134145419","id":"20190520-142038_1683726413","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:17:00+0000","dateFinished":"2019-10-17T17:17:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:251"},{"text":"%md\n#### Q1: Find the top N largest invoices by the amount (`Quantity * UnitPrice`)\n\nNote: `InvoiceNo` will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)\n\n**Sample output**\n```bash\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n```","user":"anonymous","dateUpdated":"2019-10-17T17:17:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q1: Find the top N largest invoices by the amount (<code>Quantity * UnitPrice</code>)</h4>\n<p>Note: <code>InvoiceNo</code> will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)</p>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605473_-1988178578","id":"20190520-133812_405266917","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:17:02+0000","dateFinished":"2019-10-17T17:17:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:252"},{"text":"\n\nspark.sql(\"\"\"SELECT InvoiceNo, sum(Quantity * UnitPrice) AS Amount \n             FROM retailView\n             GROUP BY InvoiceNo\n             ORDER BY Amount DESC\"\"\").show(5)\n\nval largestInvoiceDf = retailCastDf.groupBy(\"InvoiceNo\").agg(expr(\"sum(Quantity * UnitPrice)\").alias(\"Amount\")).orderBy(desc(\"Amount\"))\nlargestInvoiceDf.show(5)\nlargestInvoiceDf.createOrReplaceTempView(\"largestInvoiceView\")","user":"anonymous","dateUpdated":"2019-10-17T17:17:02+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\nonly showing top 5 rows\n\nlargestInvoiceDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, Amount: double]\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=17","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=18"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605479_-494690528","id":"20190519-215312_1016690251","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:17:02+0000","dateFinished":"2019-10-17T17:17:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:253"},{"user":"anonymous","dateUpdated":"2019-10-17T17:17:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1570548605480_-85640011","id":"20191007-145909_914572499","dateCreated":"2019-10-08T15:30:05+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254"},{"text":"%md\n#### Q2: Find the top N largest invoices by the amount and show receipt details\n\n```\n+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n```","user":"anonymous","dateUpdated":"2019-10-17T17:17:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q2: Find the top N largest invoices by the amount and show receipt details</h4>\n<pre><code>+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605481_-903145752","id":"20190520-124355_215736883","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:17:20+0000","dateFinished":"2019-10-17T17:17:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:255"},{"text":"\n\nspark.sql(\"\"\"SELECT DISTINCT(largestInvoiceView.InvoiceNo), Amount, InvoiceDate, CustomerID, Country \n             FROM largestInvoiceView\n             JOIN retailView ON retailView.InvoiceNo = largestInvoiceView.InvoiceNo \n             ORDER BY Amount DESC\"\"\").show(5)\n\nval largestDetailsDf = largestInvoiceDf.join(retailCastDf, \"InvoiceNo\").select(\"InvoiceNo\", \"Amount\", \"InvoiceDate\", \"CustomerID\", \"Country\").orderBy(desc(\"Amount\")).distinct()\nlargestDetailsDf.show(5)\nlargestDetailsDf.createOrReplaceTempView(\"largestDetailsView\")","user":"anonymous","dateUpdated":"2019-10-17T17:17:20+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\nonly showing top 5 rows\n\nlargestDetailsDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, Amount: double ... 3 more fields]\n+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=19","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=20","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=21","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=22","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=23"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605481_364103782","id":"20190520-122626_1736024345","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:17:20+0000","dateFinished":"2019-10-17T17:18:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:256"},{"text":"%md\n#### Q3: For each country, find the top N largest invoices by the amount and show receipt details\n\nUse `Window functions` and `rank()` function\n\nReadings:\n- https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n- https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\n- `Spark The Definitive Guide - page 134 - Windows Function`\n\n```\n+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n```\n\n<br>\n<br>\n################ spoiler alert ################\n**Hints**:\n- At high level, you need to create a new column which indicates amount rank by country\n  - Use `Windows` function which partition by (\"Country\") and order by amount\n  - User `Rank()` function create a new `rank` column for each row\n  - filter out rows where `rank > 2`","user":"anonymous","dateUpdated":"2019-10-17T17:18:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q3: For each country, find the top N largest invoices by the amount and show receipt details</h4>\n<p>Use <code>Window functions</code> and <code>rank()</code> function</p>\n<p>Readings:<br/>- <a href=\"https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a><br/>- <a href=\"https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\">https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a><br/>- <code>Spark The Definitive Guide - page 134 - Windows Function</code></p>\n<pre><code>+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n</code></pre>\n<br>\n<br>\n<p>################ spoiler alert ################<br/><strong>Hints</strong>:<br/>- At high level, you need to create a new column which indicates amount rank by country<br/> - Use <code>Windows</code> function which partition by (&ldquo;Country&rdquo;) and order by amount<br/> - User <code>Rank()</code> function create a new <code>rank</code> column for each row<br/> - filter out rows where <code>rank &gt; 2</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605482_-1617962075","id":"20190520-150543_915955507","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:18:34+0000","dateFinished":"2019-10-17T17:18:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"text":"spark.sql(\"\"\"SELECT InvoiceNo, Amount, InvoiceDate, CustomerID, Country \n             FROM (\n                SELECT InvoiceNo, Amount, InvoiceDate, CustomerID, Country,\n                       DENSE_RANK() OVER (PARTITION BY Country ORDER BY Amount DESC) AS AmountRank \n                FROM largestDetailsView)\n             WHERE AmountRank <= 2\"\"\").show(10)\n\nimport org.apache.spark.sql.expressions.Window\n\nval windowSpec = Window.partitionBy(\"Country\").orderBy(col(\"Amount\").desc)\nval amountDenseRank = dense_rank().over(windowSpec)\nval top2ByCountry = largestDetailsDf.withColumn(\"AmountRank\",amountDenseRank).where(col(\"AmountRank\") <= 2).select(col(\"InvoiceNo\"), col(\"Amount\"), col(\"InvoiceDate\"), col(\"CustomerID\"), col(\"Country\"))\ntop2ByCountry.show(10)","user":"anonymous","dateUpdated":"2019-10-17T17:18:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\nonly showing top 10 rows\n\nimport org.apache.spark.sql.expressions.Window\nwindowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@6e147140\namountDenseRank: org.apache.spark.sql.Column = DENSE_RANK() OVER (PARTITION BY Country ORDER BY Amount DESC NULLS LAST unspecifiedframe$())\ntop2ByCountry: org.apache.spark.sql.DataFrame = [InvoiceNo: string, Amount: double ... 3 more fields]\n+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\nonly showing top 10 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=24","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=25","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=26","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=27","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=28","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=29","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=30","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=31","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=32","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=33","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=34","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=35"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605486_-1031424332","id":"20190520-125029_1350468290","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:18:34+0000","dateFinished":"2019-10-17T17:19:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:258"},{"text":"%md\n\n#### Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.\n\n\n```bash\ndailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n```\n\n```bash\nweeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n```\n\nReadings\n- https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/","user":"anonymous","dateUpdated":"2019-10-17T17:19:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.</h4>\n<pre><code class=\"bash\">dailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n</code></pre>\n<pre><code class=\"bash\">weeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n</code></pre>\n<p>Readings<br/>- <a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\">https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548605487_-1031035595","id":"20190520-140931_1510736707","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:19:30+0000","dateFinished":"2019-10-17T17:19:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:259"},{"text":"spark.sql(\"\"\"\n        SELECT DISTINCT(start), round(AmountNotRounded, 3) AS Amount\n        FROM (\n             SELECT to_timestamp(to_date(InvoiceDate + INTERVAL 5 HOURS)) - INTERVAL 5 HOURS AS start,\n                    SUM(Quantity * UnitPrice)\n                        OVER (PARTITION BY to_timestamp(to_date(InvoiceDate + INTERVAL 5 HOURS)))\n                        AS AmountNotRounded\n             FROM retailView\n            )\n        ORDER BY start ASC\"\"\").show(5)\n\nval dailySalesDf = retailCastDf.groupBy(window(col(\"InvoiceDate\"), \"1 day\", \"1 day\", \"19 hours\").alias(\"time\")).agg(round(expr(\"sum(Quantity * UnitPrice)\"), 3).alias(\"Amount\")).select(expr(\"time.start\"), expr(\"Amount\")).orderBy(\"time.start\")\ndailySalesDf.show(5)\ndailySalesDf.createOrReplaceTempView(\"dailySales\")","user":"anonymous","dateUpdated":"2019-10-17T18:27:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+--------+\n|              start|  Amount|\n+-------------------+--------+\n|2010-11-30 19:00:00|58635.56|\n|2010-12-01 19:00:00|45666.63|\n|2010-12-02 19:00:00|46161.11|\n|2010-12-04 19:00:00|31383.95|\n|2010-12-05 19:00:00|53860.18|\n+-------------------+--------+\nonly showing top 5 rows\n\ndailySalesDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: timestamp, Amount: double]\n+-------------------+--------+\n|              start|  Amount|\n+-------------------+--------+\n|2010-11-30 19:00:00|58635.56|\n|2010-12-01 19:00:00|45666.63|\n|2010-12-02 19:00:00|46161.11|\n|2010-12-04 19:00:00|31383.95|\n|2010-12-05 19:00:00|53860.18|\n+-------------------+--------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=69","http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=70"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605487_-295533650","id":"20190520-181045_1661878813","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T18:27:18+0000","dateFinished":"2019-10-17T18:27:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:260"},{"text":"%sql\nselect to_date(start), `Amount` from dailySales","user":"anonymous","dateUpdated":"2019-10-17T19:15:34+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"lineChart","height":326,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"to_date(dailysales.`start`)":"string","Amount":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default","isDateFormat":false},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"pieChart":{}},"commonSetting":{},"keys":[{"name":"to_date(dailysales.`start`)","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"Amount","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"to_date(dailysales.`start`)\tAmount\n2010-11-30\t58635.56\n2010-12-01\t45666.63\n2010-12-02\t46161.11\n2010-12-04\t31383.95\n2010-12-05\t53860.18\n2010-12-06\t45059.05\n2010-12-07\t44189.84\n2010-12-08\t51501.75\n2010-12-09\t58435.29\n2010-12-11\t17240.92\n2010-12-12\t35379.34\n2010-12-13\t42843.29\n2010-12-14\t29443.69\n2010-12-15\t45654.69\n2010-12-16\t46213.85\n2010-12-18\t7517.31\n2010-12-19\t24741.75\n2010-12-20\t47097.94\n2010-12-21\t6134.57\n2010-12-22\t11796.31\n2011-01-03\t14950.48\n2011-01-04\t-1566.23\n2011-01-05\t37392.74\n2011-01-06\t27233.14\n2011-01-08\t15710.8\n2011-01-09\t24191.64\n2011-01-10\t67817.13\n2011-01-11\t23958.78\n2011-01-12\t20533.54\n2011-01-13\t47377.26\n2011-01-15\t7116.61\n2011-01-16\t29256.0\n2011-01-17\t18680.8\n2011-01-18\t25585.81\n2011-01-19\t17773.67\n2011-01-20\t32200.68\n2011-01-22\t10285.95\n2011-01-23\t25555.62\n2011-01-24\t27971.52\n2011-01-25\t19493.32\n2011-01-26\t20910.79\n2011-01-27\t18749.12\n2011-01-29\t6456.44\n2011-01-30\t22364.65\n2011-01-31\t28433.22\n2011-02-01\t21048.45\n2011-02-02\t23040.58\n2011-02-03\t25298.17\n2011-02-05\t3457.11\n2011-02-06\t25525.99\n2011-02-07\t20728.14\n2011-02-08\t16692.58\n2011-02-09\t13203.94\n2011-02-10\t20610.88\n2011-02-12\t5535.4\n2011-02-13\t26222.03\n2011-02-14\t36842.58\n2011-02-15\t24730.81\n2011-02-16\t25996.24\n2011-02-17\t16294.03\n2011-02-19\t9578.89\n2011-02-20\t23807.83\n2011-02-21\t32292.62\n2011-02-22\t26792.76\n2011-02-23\t22655.83\n2011-02-24\t18029.84\n2011-02-26\t9491.05\n2011-02-27\t21753.68\n2011-02-28\t25471.71\n2011-03-01\t18296.45\n2011-03-02\t35543.63\n2011-03-03\t19773.86\n2011-03-05\t9596.23\n2011-03-06\t30525.58\n2011-03-07\t25017.47\n2011-03-08\t21907.12\n2011-03-09\t25431.39\n2011-03-10\t22161.78\n2011-03-12\t4137.62\n2011-03-13\t25864.59\n2011-03-14\t20660.03\n2011-03-15\t21182.64\n2011-03-16\t37377.91\n2011-03-17\t18196.8\n2011-03-19\t21980.64\n2011-03-20\t16370.27\n2011-03-21\t31312.35\n2011-03-22\t24029.07\n2011-03-23\t36192.97\n2011-03-24\t31025.16\n2011-03-26\t8979.98\n2011-03-27\t19207.03\n2011-03-28\t70531.47\n2011-03-29\t31489.25\n2011-03-30\t30019.18\n2011-03-31\t25376.68\n2011-04-02\t6878.1\n2011-04-03\t25073.02\n2011-04-04\t28353.83\n2011-04-05\t17279.35\n2011-04-06\t15927.27\n2011-04-07\t25600.87\n2011-04-09\t9363.88\n2011-04-10\t22110.31\n2011-04-11\t25124.25\n2011-04-12\t23898.2\n2011-04-13\t34682.12\n2011-04-14\t28940.591\n2011-04-16\t12704.3\n2011-04-17\t32185.61\n2011-04-18\t23837.65\n2011-04-19\t28239.39\n2011-04-20\t30114.08\n2011-04-21\t1084.52\n2011-04-25\t30585.54\n2011-04-26\t25590.56\n2011-04-27\t21683.16\n2011-04-28\t-441.26\n2011-04-30\t6964.66\n2011-05-02\t19617.86\n2011-05-03\t27462.3\n2011-05-04\t28298.9\n2011-05-05\t35770.73\n2011-05-06\t395.6\n2011-05-07\t18808.92\n2011-05-08\t26060.43\n2011-05-09\t45564.12\n2011-05-10\t33240.36\n2011-05-11\t57792.84\n2011-05-12\t32863.2\n2011-05-14\t9924.28\n2011-05-15\t25279.77\n2011-05-16\t53603.83\n2011-05-17\t34337.29\n2011-05-18\t32477.98\n2011-05-19\t28127.29\n2011-05-21\t24205.37\n2011-05-22\t30739.55\n2011-05-23\t37028.91\n2011-05-24\t24152.28\n2011-05-25\t27396.67\n2011-05-26\t34044.11\n2011-05-28\t7208.3\n2011-05-30\t21967.96\n2011-05-31\t20191.2\n2011-06-01\t31811.72\n2011-06-02\t17441.29\n2011-06-04\t25520.35\n2011-06-05\t16791.39\n2011-06-06\t37644.3\n2011-06-07\t42940.91\n2011-06-08\t42613.79\n2011-06-09\t25442.62\n2011-06-11\t12483.86\n2011-06-12\t20372.93\n2011-06-13\t40211.93\n2011-06-14\t46139.18\n2011-06-15\t33354.75\n2011-06-16\t21577.7\n2011-06-18\t22360.01\n2011-06-19\t33493.4\n2011-06-20\t22730.01\n2011-06-21\t21794.94\n2011-06-22\t22835.44\n2011-06-23\t10057.75\n2011-06-25\t6175.17\n2011-06-26\t16823.86\n2011-06-27\t34704.64\n2011-06-28\t21775.43\n2011-06-29\t42814.52\n2011-06-30\t14191.85\n2011-07-02\t5977.14\n2011-07-03\t44154.75\n2011-07-04\t40334.97\n2011-07-05\t26279.58\n2011-07-06\t30699.19\n2011-07-07\t27498.61\n2011-07-09\t5692.07\n2011-07-10\t22429.53\n2011-07-11\t25892.04\n2011-07-12\t11612.05\n2011-07-13\t31581.4\n2011-07-14\t15473.49\n2011-07-16\t17174.66\n2011-07-17\t28443.27\n2011-07-18\t49316.78\n2011-07-19\t27305.41\n2011-07-20\t30373.5\n2011-07-21\t20598.8\n2011-07-23\t26476.2\n2011-07-24\t26687.65\n2011-07-25\t21271.301\n2011-07-26\t25568.45\n2011-07-27\t55402.68\n2011-07-28\t18398.41\n2011-07-30\t33486.36\n2011-07-31\t21362.84\n2011-08-01\t14947.27\n2011-08-02\t27075.02\n2011-08-03\t58839.98\n2011-08-04\t23486.97\n2011-08-06\t7464.12\n2011-08-07\t19987.15\n2011-08-08\t26623.2\n2011-08-09\t27474.22\n2011-08-10\t72132.79\n2011-08-11\t10049.48\n2011-08-13\t5150.18\n2011-08-14\t17205.54\n2011-08-15\t19103.71\n2011-08-16\t33307.32\n2011-08-17\t68981.98\n2011-08-18\t17577.13\n2011-08-20\t14549.21\n2011-08-21\t27978.41\n2011-08-22\t25756.3\n2011-08-23\t37074.9\n2011-08-24\t22103.35\n2011-08-25\t25905.76\n2011-08-27\t10784.78\n2011-08-29\t31640.9\n2011-08-30\t16118.0\n2011-08-31\t36580.21\n2011-09-01\t42461.46\n2011-09-03\t17018.49\n2011-09-04\t36844.04\n2011-09-05\t28052.62\n2011-09-06\t34125.65\n2011-09-07\t26353.47\n2011-09-08\t29672.22\n2011-09-10\t35465.47\n2011-09-11\t29039.31\n2011-09-12\t54828.45\n2011-09-13\t23360.66\n2011-09-14\t62766.87\n2011-09-15\t26035.0\n2011-09-17\t15692.33\n2011-09-18\t46212.21\n2011-09-19\t109286.21\n2011-09-20\t42944.07\n2011-09-21\t56361.02\n2011-09-22\t40142.29\n2011-09-24\t31210.921\n2011-09-25\t28642.271\n2011-09-26\t35752.16\n2011-09-27\t43383.04\n2011-09-28\t43221.13\n2011-09-29\t44236.05\n2011-10-01\t11623.58\n2011-10-02\t64214.78\n2011-10-03\t48240.84\n2011-10-04\t75244.43\n2011-10-05\t58840.46\n2011-10-06\t44003.84\n2011-10-08\t11922.24\n2011-10-09\t44265.89\n2011-10-10\t38267.75\n2011-10-11\t29302.85\n2011-10-12\t37067.17\n2011-10-13\t35225.54\n2011-10-15\t21605.44\n2011-10-16\t47064.14\n2011-10-17\t44637.84\n2011-10-18\t36003.43\n2011-10-19\t60158.81\n2011-10-20\t63595.59\n2011-10-22\t12302.41\n2011-10-23\t38407.72\n2011-10-24\t40807.49\n2011-10-25\t37842.08\n2011-10-26\t45962.33\n2011-10-27\t41077.29\n2011-10-29\t34545.28\n2011-10-30\t48475.45\n2011-10-31\t28741.55\n2011-11-01\t45239.06\n2011-11-02\t60774.05\n2011-11-03\t62124.26\n2011-11-05\t42912.4\n2011-11-06\t70001.08\n2011-11-07\t56647.66\n2011-11-08\t62599.43\n2011-11-09\t66605.45\n2011-11-10\t57186.3\n2011-11-12\t33520.22\n2011-11-13\t112141.11\n2011-11-14\t60594.23\n2011-11-15\t64408.7\n2011-11-16\t57904.93\n2011-11-17\t50456.59\n2011-11-19\t34902.01\n2011-11-20\t48302.5\n2011-11-21\t62307.32\n2011-11-22\t78480.7\n2011-11-23\t47275.9\n2011-11-24\t51247.1\n2011-11-26\t20571.5\n2011-11-27\t55442.02\n2011-11-28\t72219.2\n2011-11-29\t59150.98\n2011-11-30\t49852.27\n2011-12-01\t58644.74\n2011-12-03\t24565.78\n2011-12-04\t57751.32\n2011-12-05\t54228.37\n2011-12-06\t75076.22\n2011-12-07\t80405.07\n2011-12-08\t33144.24\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=60"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605488_1776034881","id":"20190520-140933_785400989","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T18:00:15+0000","dateFinished":"2019-10-17T18:00:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:261"},{"text":"","user":"anonymous","dateUpdated":"2019-10-17T17:29:07+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1571333342977_1258502928","id":"20191017-172902_1819898918","dateCreated":"2019-10-17T17:29:02+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:262"},{"text":"\n\nval weeklySalesDf = retailCastDf.groupBy(window(col(\"InvoiceDate\"), \"1 week\", \"1 week\", \"6 days 19 hours\").alias(\"time\")).agg(round(expr(\"sum(Quantity * UnitPrice)\"),3).alias(\"Amount\")).orderBy(\"time\").select(expr(\"time.start\"), expr(\"Amount\"))\nweeklySalesDf.show(5)\nweeklySalesDf.createOrReplaceTempView(\"weeklySales\")","user":"anonymous","dateUpdated":"2019-10-17T17:27:25+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"weeklySalesDf: org.apache.spark.sql.DataFrame = [start: timestamp, Amount: double]\n+-------------------+---------+\n|              start|   Amount|\n+-------------------+---------+\n|2010-11-24 19:00:00| 58635.56|\n|2010-12-01 19:00:00|266320.76|\n|2010-12-08 19:00:00|234844.28|\n|2010-12-15 19:00:00|177360.11|\n|2010-12-22 19:00:00| 11796.31|\n+-------------------+---------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=51"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605489_120157711","id":"20190520-140933_428817963","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:27:25+0000","dateFinished":"2019-10-17T17:27:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:263"},{"text":"%sql\nselect to_date(start), `Amount` from weeklySales","user":"anonymous","dateUpdated":"2019-10-17T19:15:34+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"to_date(weeklysales.`start`)":"string","sum(amount)":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"to_date(weeklysales.`start`)","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"Amount","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"to_date(weeklysales.`start`)\tAmount\n2010-11-24\t58635.56\n2010-12-01\t266320.76\n2010-12-08\t234844.28\n2010-12-15\t177360.11\n2010-12-22\t11796.31\n2010-12-29\t13384.25\n2011-01-05\t196304.23\n2011-01-12\t148550.02\n2011-01-19\t133280.76\n2011-01-26\t117962.67\n2011-02-02\t114742.57\n2011-02-09\t127145.64\n2011-02-16\t134762.37\n2011-02-23\t115698.56\n2011-03-02\t142363.89\n2011-03-09\t119438.05\n2011-03-16\t149267.04\n2011-03-23\t197425.86\n2011-03-30\t132980.16\n2011-04-06\t122024.78\n2011-04-13\t160589.661\n2011-04-20\t87374.7\n2011-04-27\t75286.72\n2011-05-04\t188139.06\n2011-05-11\t213801.21\n2011-05-18\t176731.38\n2011-05-25\t110808.24\n2011-06-01\t172149.96\n2011-06-08\t187264.31\n2011-06-15\t155310.81\n2011-06-22\t112372.29\n2011-06-29\t173752.81\n2011-07-06\t123823.49\n2011-07-13\t169295.01\n2011-07-20\t150975.901\n2011-07-27\t170672.58\n2011-08-03\t163875.64\n2011-08-10\t156949.02\n2011-08-17\t191917.93\n2011-08-24\t106552.79\n2011-08-31\t195082.47\n2011-09-07\t198719.58\n2011-09-14\t302936.69\n2011-09-21\t235491.702\n2011-09-28\t286780.81\n2011-10-05\t226603.03\n2011-10-12\t221603.56\n2011-10-19\t253114.1\n2011-10-26\t244040.96\n2011-11-02\t355058.88\n2011-11-09\t394456.01\n2011-11-16\t332354.05\n2011-11-23\t305906.7\n2011-11-30\t320118.7\n2011-12-07\t113549.31\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=49"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548605489_995813130","id":"20190520-212256_1274740776","dateCreated":"2019-10-08T15:30:05+0000","dateStarted":"2019-10-17T17:26:29+0000","dateFinished":"2019-10-17T17:26:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:264"},{"text":"spark.sql(\"\"\"\n        SELECT DISTINCT(start), round(AmountNotRounded, 3) AS Amount\n        FROM (\n             SELECT to_timestamp(date_sub(InvoiceDate + INTERVAL 5 HOUR, difference)) - INTERVAL 5 HOUR AS start,\n                    SUM(Quantity * UnitPrice)\n                        OVER (\n                            PARTITION BY to_timestamp(date_sub(InvoiceDate + INTERVAL 5 HOUR, difference)) - INTERVAL 5 HOUR\n                        )\n                        AS AmountNotRounded\n             FROM ( SELECT InvoiceDate, Quantity, UnitPrice,\n                           CASE WHEN dayofweek(InvoiceDate + INTERVAL 5 HOUR) >= 5 THEN dayofweek(InvoiceDate + INTERVAL 5 HOUR) - 5\n                                ELSE 7 + dayofweek(InvoiceDate + INTERVAL 5 HOUR) - 5\n                                END AS difference\n                    FROM retailView\n             )\n        )\n        ORDER BY start ASC\"\"\").show(5)","user":"anonymous","dateUpdated":"2019-10-17T17:45:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+---------+\n|              start|   Amount|\n+-------------------+---------+\n|2010-11-24 19:00:00| 58635.56|\n|2010-12-01 19:00:00|266320.76|\n|2010-12-08 19:00:00|234844.28|\n|2010-12-15 19:00:00|177360.11|\n|2010-12-22 19:00:00| 11796.31|\n+-------------------+---------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://jarvis-bootcamp-m.us-east1-d.c.big-booking-244019.internal:4040/jobs/job?id=55"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1571239329216_-1329110346","id":"20191016-152209_843603249","dateCreated":"2019-10-16T15:22:09+0000","dateStarted":"2019-10-17T17:45:44+0000","dateFinished":"2019-10-17T17:45:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1571333314051_-1067477917","id":"20191017-172834_1563095141","dateCreated":"2019-10-17T17:28:34+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:266"}],"name":"Spark DataFrame project","id":"2EQ5GJU52","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}